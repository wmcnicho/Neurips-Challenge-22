/work/nashokkumar_umass_edu/nipschal/Neurips-Challenge-22/predict_graph_embed_small.py:540: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(dataset_tensor[:, 1, :].clone().detach(), dtype=torch.long)
PermutedDKT
Number of questions:  1855
Number of students:  6468
Number of concepts: 1076
Using batch size: 64
Using batch size: 64
AFTER LOADING THE MODEL
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   39973 KB |   39973 KB |   39973 KB |       0 B  |
|       from large pool |   39967 KB |   39967 KB |   39967 KB |       0 B  |
|       from small pool |       6 KB |       6 KB |       6 KB |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |   39973 KB |   39973 KB |   39973 KB |       0 B  |
|       from large pool |   39967 KB |   39967 KB |   39967 KB |       0 B  |
|       from small pool |       6 KB |       6 KB |       6 KB |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   63488 KB |   63488 KB |   63488 KB |       0 B  |
|       from large pool |   61440 KB |   61440 KB |   61440 KB |       0 B  |
|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   23514 KB |   23516 KB |   53176 KB |   29662 KB |
|       from large pool |   21473 KB |   21473 KB |   51133 KB |   29660 KB |
|       from small pool |    2041 KB |    2043 KB |    2043 KB |       2 KB |
|---------------------------------------------------------------------------|
| Allocations           |      14    |      14    |      14    |       0    |
|       from large pool |      11    |      11    |      11    |       0    |
|       from small pool |       3    |       3    |       3    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |      14    |      14    |      14    |       0    |
|       from large pool |      11    |      11    |      11    |       0    |
|       from small pool |       3    |       3    |       3    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       4    |       4    |       4    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |       1    |       1    |       1    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Successfull in data prepration!
Successfully loaded the optimizer
======== Epoch 1 / 10 ========
Training...
Step: 0
batch input size:batch input size:  torch.Size([32, 1855])torch.Size([32, 1855])  Device: Device:0
1
tensor([[ 0.0017, -0.0003,  0.0018,  ..., -0.0019, -0.0019,  0.0009],
        [ 0.0001, -0.0014, -0.0009,  ..., -0.0007,  0.0002,  0.0012],
        [ 0.0010, -0.0005,  0.0022,  ..., -0.0010, -0.0009, -0.0017],
        ...,
        [ 0.0022, -0.0012, -0.0018,  ..., -0.0013,  0.0007, -0.0019],
        [ 0.0021, -0.0010,  0.0016,  ..., -0.0002,  0.0019,  0.0018],
        [-0.0016, -0.0019, -0.0018,  ...,  0.0016, -0.0013, -0.0004]],
       device='cuda:1', grad_fn=<BroadcastBackward>)
tensor([[ 0.0017, -0.0003,  0.0018,  ..., -0.0019, -0.0019,  0.0009],
        [ 0.0001, -0.0014, -0.0009,  ..., -0.0007,  0.0002,  0.0012],
        [ 0.0010, -0.0005,  0.0022,  ..., -0.0010, -0.0009, -0.0017],
        ...,
        [ 0.0022, -0.0012, -0.0018,  ..., -0.0013,  0.0007, -0.0019],
        [ 0.0021, -0.0010,  0.0016,  ..., -0.0002,  0.0019,  0.0018],
        [-0.0016, -0.0019, -0.0018,  ...,  0.0016, -0.0013, -0.0004]],
       device='cuda:0', grad_fn=<BroadcastBackward>)
Traceback (most recent call last):
  File "/work/nashokkumar_umass_edu/nipschal/Neurips-Challenge-22/predict_graph_embed_small.py", line 604, in <module>
    main()
  File "/work/nashokkumar_umass_edu/nipschal/Neurips-Challenge-22/predict_graph_embed_small.py", line 589, in main
    model, epoch_train_loss, epoch_val_loss = train(epochs, dkt_model, train_dataloader, val_dataloader, optimizer, scheduler) # add val_dataloader later
  File "/work/nashokkumar_umass_edu/nipschal/Neurips-Challenge-22/predict_graph_embed_small.py", line 433, in train
    loss = model(b_input_ids, b_labels, epoch_i+1)
  File "/work/nashokkumar_umass_edu/.conda/envs/nistorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/work/nashokkumar_umass_edu/.conda/envs/nistorch/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py", line 168, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/work/nashokkumar_umass_edu/.conda/envs/nistorch/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py", line 178, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/work/nashokkumar_umass_edu/.conda/envs/nistorch/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/work/nashokkumar_umass_edu/.conda/envs/nistorch/lib/python3.10/site-packages/torch/_utils.py", line 461, in reraise
    raise exception
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/work/nashokkumar_umass_edu/.conda/envs/nistorch/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/work/nashokkumar_umass_edu/.conda/envs/nistorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/work/nashokkumar_umass_edu/nipschal/Neurips-Challenge-22/predict_graph_embed_small.py", line 328, in forward
    loss_masked = (raw_loss * mask).mean()
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!